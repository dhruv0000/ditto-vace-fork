{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Ditto Training Pipeline (Global Style)\n",
                "\n",
                "This notebook sets up the environment and runs the Ditto training pipeline on the global style editing subset (videos/global_style1 & videos/global_style2) of Ditto-1M on Google Colab Pro."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Mount Google Drive\n",
                "Mount your Google Drive to save checkpoints and access the dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Setup Environment\n",
                "Clone the repository and install necessary dependencies."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!git clone https://github.com/dhruv0000/ditto-vace-fork.git\n",
                "%cd ditto-vace-fork\n",
                "!pip install -r requirements.txt\n",
                "!pip install -e .\n",
                "!pip install accelerate"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Setup\n",
                "Download the global style dataset and metadata from Hugging Face."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title Data Download/Setup (Global Style)\n",
                "import os\n",
                "from huggingface_hub import snapshot_download\n",
                "\n",
                "# Store dataset and caches on Google Drive so they persist across sessions\n",
                "os.environ[\"HF_HOME\"] = \"/content/drive/MyDrive/hf_home\"\n",
                "os.environ[\"HF_DATASETS_CACHE\"] = \"/content/drive/MyDrive/hf_home\"\n",
                "\n",
                "dataset_root = \"/content/drive/MyDrive/Ditto-1M\"\n",
                "\n",
                "# If True and metadata already exists in Drive, skip re-downloading\n",
                "skip_download_if_exists = True  #@param {type:\"boolean\"}\n",
                "\n",
                "# Comma-separated list of tar indices to download for each subset, e.g. \"01,02\"\n",
                "tar_indices = \"01\"  #@param {type:\"string\"}\n",
                "\n",
                "# Whether to download archives for each subset\n",
                "download_source_archives = True  #@param {type:\"boolean\"}\n",
                "download_global_style1_archives = True  #@param {type:\"boolean\"}\n",
                "download_global_style2_archives = True  #@param {type:\"boolean\"}\n",
                "\n",
                "metadata_file = os.path.join(dataset_root, \"csvs_for_DiffSynth\", \"global_style.csv\")\n",
                "if skip_download_if_exists and os.path.isdir(dataset_root) and os.path.exists(metadata_file):\n",
                "    print(f\"Found existing Ditto-1M metadata in Drive at {dataset_root}. Skipping download.\")\n",
                "else:\n",
                "    # Download metadata JSON and CSV for the global style task\n",
                "    snapshot_download(\n",
                "        repo_id=\"QingyanBai/Ditto-1M\",\n",
                "        repo_type=\"dataset\",\n",
                "        local_dir=dataset_root,\n",
                "        allow_patterns=[\n",
                "            \"training_metadata/global_style.json\",\n",
                "            \"csvs_for_DiffSynth/global_style.csv\",\n",
                "        ],\n",
                "    )\n",
                "\n",
                "    # Build list of tar archives to download\n",
                "    indices = [s.strip() for s in tar_indices.split(\",\") if s.strip()]\n",
                "    subsets = []\n",
                "    if download_source_archives:\n",
                "        subsets.append(\"source\")\n",
                "    if download_global_style1_archives:\n",
                "        subsets.append(\"global_style1\")\n",
                "    if download_global_style2_archives:\n",
                "        subsets.append(\"global_style2\")\n",
                "\n",
                "    tar_allow_patterns = []\n",
                "    for subset in subsets:\n",
                "        for idx in indices:\n",
                "            tar_allow_patterns.append(f\"videos/{subset}/{subset}.tar.gz.{idx}\")\n",
                "\n",
                "    if tar_allow_patterns:\n",
                "        print(\"Downloading tar archives:\")\n",
                "        for p in tar_allow_patterns:\n",
                "            print(\"  \", p)\n",
                "        snapshot_download(\n",
                "            repo_id=\"QingyanBai/Ditto-1M\",\n",
                "            repo_type=\"dataset\",\n",
                "            local_dir=dataset_root,\n",
                "            allow_patterns=tar_allow_patterns,\n",
                "        )\n",
                "    else:\n",
                "        print(\"No tar archives selected for download (tar_indices or subset flags empty).\")\n",
                "\n",
                "    print(\"Metadata and selected video archives downloaded to Google Drive.\")\n",
                "\n",
                "# Optionally extract the split archives from Drive\n",
                "extract_archives = True  #@param {type:\"boolean\"}\n",
                "\n",
                "if extract_archives:\n",
                "    import glob\n",
                "\n",
                "    cwd = os.getcwd()\n",
                "    try:\n",
                "        for subset in [\"source\", \"global_style1\", \"global_style2\"]:\n",
                "            subset_dir = os.path.join(dataset_root, \"videos\", subset)\n",
                "            if not os.path.isdir(subset_dir):\n",
                "                print(f\"Directory {subset_dir} does not exist, skipping.\")\n",
                "                continue\n",
                "\n",
                "            os.chdir(subset_dir)\n",
                "            part_pattern = f\"{subset}.tar.gz.*\"\n",
                "            part_files = sorted(glob.glob(part_pattern))\n",
                "            if not part_files:\n",
                "                print(f\"No split archives found for {subset}, skipping extraction.\")\n",
                "                continue\n",
                "\n",
                "            print(f\"Extracting {subset} from {len(part_files)} tar archives...\")\n",
                "            os.system(f\"cat {part_pattern} | tar -zxv\")\n",
                "    finally:\n",
                "        os.chdir(cwd)\n",
                "\n",
                "print(\"Data setup complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title Build mini CSV from extracted videos\n",
                "import os\n",
                "import pandas as pd\n",
                "\n",
                "dataset_root = \"/content/drive/MyDrive/Ditto-1M\"\n",
                "csv_path = os.path.join(dataset_root, \"csvs_for_DiffSynth\", \"global_style.csv\")\n",
                "\n",
                "print(\"Loading full global_style CSV from:\", csv_path)\n",
                "df = pd.read_csv(csv_path)\n",
                "\n",
                "videos_root = os.path.join(dataset_root, \"videos\")\n",
                "existing_files = set()\n",
                "for root, _, files in os.walk(videos_root):\n",
                "    for fname in files:\n",
                "        rel_path = os.path.relpath(os.path.join(root, fname), dataset_root)\n",
                "        existing_files.add(rel_path)\n",
                "\n",
                "print(\"Total existing video files under videos/:\", len(existing_files))\n",
                "\n",
                "required_cols = [\"video\", \"vace_video\"]\n",
                "for col in required_cols:\n",
                "    if col not in df.columns:\n",
                "        raise ValueError(f\"Required column '{col}' not found in CSV. Available columns: {df.columns.tolist()}\")\n",
                "\n",
                "mask = df[\"video\"].isin(existing_files) & df[\"vace_video\"].isin(existing_files)\n",
                "df_existing = df[mask].reset_index(drop=True)\n",
                "print(\"Rows with both video and vace_video present:\", len(df_existing))\n",
                "\n",
                "n_samples = 20000  #@param {type:\"integer\"}\n",
                "if n_samples > len(df_existing):\n",
                "    n_samples = len(df_existing)\n",
                "    print(f\"Requested n_samples larger than available; using {n_samples} rows.\")\n",
                "\n",
                "if n_samples > 0:\n",
                "    df_mini = df_existing.sample(n=n_samples, random_state=0)\n",
                "else:\n",
                "    df_mini = df_existing\n",
                "\n",
                "mini_csv_path = os.path.join(dataset_root, \"csvs_for_DiffSynth\", \"global_style_mini.csv\")\n",
                "df_mini.to_csv(mini_csv_path, index=False)\n",
                "print(\"Saved mini CSV to:\", mini_csv_path)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Configuration\n",
                "Configure the training parameters for the global style task. The default model is `Wan-AI/Wan2.1-VACE-1.3B`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title Training Configuration\n",
                "\n",
                "dataset_base_path = \"/content/drive/MyDrive/Ditto-1M/videos\" #@param {type:\"string\"}\n",
                "dataset_metadata_path = \"/content/drive/MyDrive/Ditto-1M/csvs_for_DiffSynth/global_style_mini.csv\" #@param {type:\"string\"}\n",
                "output_path = \"/content/drive/MyDrive/exps/ditto\" #@param {type:\"string\"}\n",
                "model_id = \"Wan-AI/Wan2.1-VACE-1.3B\" #@param {type:\"string\"}\n",
                "num_epochs = 5 #@param {type:\"integer\"}\n",
                "learning_rate = \"1e-4\" #@param {type:\"string\"}\n",
                "\n",
                "print(f\"Configuration:\")\n",
                "print(f\"  Dataset Base Path: {dataset_base_path}\")\n",
                "print(f\"  Metadata Path: {dataset_metadata_path}\")\n",
                "print(f\"  Output Path: {output_path}\")\n",
                "print(f\"  Model ID: {model_id}\")\n",
                "print(f\"  Epochs: {num_epochs}\")\n",
                "print(f\"  Learning Rate: {learning_rate}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Run Training\n",
                "Execute the training script with the configured parameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!chmod +x train.sh\n",
                "!./train.sh \\\n",
                "  --dataset_base_path \"{dataset_base_path}\" \\\n",
                "  --dataset_metadata_path \"{dataset_metadata_path}\" \\\n",
                "  --output_path \"{output_path}\" \\\n",
                "  --model_id \"{model_id}\" \\\n",
                "  --num_epochs \"{num_epochs}\" \\\n",
                "  --learning_rate \"{learning_rate}\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

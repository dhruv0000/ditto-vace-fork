{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Ditto Training Pipeline (Global Style)\n",
                "\n",
                "This notebook sets up the environment and runs the Ditto training pipeline on the global style editing subset (videos/global_style1 & videos/global_style2) of Ditto-1M on Google Colab Pro."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Mount Google Drive\n",
                "Mount your Google Drive to save checkpoints and access the dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Setup Environment\n",
                "Clone the repository and install necessary dependencies."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!git clone https://github.com/dhruv0000/ditto-vace-fork.git\n",
                "%cd ditto-vace-fork\n",
                "!pip install -r requirements.txt\n",
                "!pip install -e .\n",
                "!pip install accelerate"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Setup\n",
                "Download the global style dataset and metadata from Hugging Face."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title Data Download/Setup (Global Style)\n",
                "import os\n",
                "from huggingface_hub import snapshot_download\n",
                "\n",
                "dataset_root = \"./Ditto-1M\"\n",
                "\n",
                "# Download metadata JSON and CSV for the global style task\n",
                "snapshot_download(\n",
                "    repo_id=\"QingyanBai/Ditto-1M\",\n",
                "    repo_type=\"dataset\",\n",
                "    local_dir=dataset_root,\n",
                "    allow_patterns=[\n",
                "        \"training_metadata/global_style.json\",\n",
                "        \"csvs_for_DiffSynth/global_style.csv\",\n",
                "    ],\n",
                ")\n",
                "\n",
                "# Download source and global style video archives\n",
                "snapshot_download(\n",
                "    repo_id=\"QingyanBai/Ditto-1M\",\n",
                "    repo_type=\"dataset\",\n",
                "    local_dir=dataset_root,\n",
                "    allow_patterns=[\n",
                "        \"videos/source/*\",\n",
                "        \"videos/global_style1/*\",\n",
                "        \"videos/global_style2/*\",\n",
                "    ],\n",
                ")\n",
                "\n",
                "print(\"Metadata and video archives downloaded.\")\n",
                "\n",
                "# Optionally extract the split archives for global_style1 and global_style2\n",
                "extract_archives = True  #@param {type:\"boolean\"}\n",
                "\n",
                "if extract_archives:\n",
                "    import glob\n",
                "\n",
                "    cwd = os.getcwd()\n",
                "    try:\n",
                "        for subset in [\"global_style1\", \"global_style2\"]:\n",
                "            subset_dir = os.path.join(dataset_root, \"videos\", subset)\n",
                "            if not os.path.isdir(subset_dir):\n",
                "                print(f\"Directory {subset_dir} does not exist, skipping.\")\n",
                "                continue\n",
                "\n",
                "            os.chdir(subset_dir)\n",
                "            part_pattern = f\"{subset}.tar.gz.*\"\n",
                "            part_files = sorted(glob.glob(part_pattern))\n",
                "            if not part_files:\n",
                "                print(f\"No split archives found for {subset}, skipping extraction.\")\n",
                "                continue\n",
                "\n",
                "            print(f\"Extracting {subset} from {len(part_files)} parts...\")\n",
                "            os.system(f\"cat {part_pattern} | tar -zxv\")\n",
                "    finally:\n",
                "        os.chdir(cwd)\n",
                "\n",
                "print(\"Data setup complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Configuration\n",
                "Configure the training parameters for the global style task. The default model is `Wan-AI/Wan2.1-VACE-1.3B`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title Training Configuration\n",
                "\n",
                "dataset_base_path = \"./Ditto-1M/videos\" #@param {type:\"string\"}\n",
                "dataset_metadata_path = \"./Ditto-1M/csvs_for_DiffSynth/global_style.csv\" #@param {type:\"string\"}\n",
                "output_path = \"/content/drive/MyDrive/exps/ditto\" #@param {type:\"string\"}\n",
                "model_id = \"Wan-AI/Wan2.1-VACE-1.3B\" #@param {type:\"string\"}\n",
                "num_epochs = 5 #@param {type:\"integer\"}\n",
                "learning_rate = \"1e-4\" #@param {type:\"string\"}\n",
                "\n",
                "print(f\"Configuration:\")\n",
                "print(f\"  Dataset Base Path: {dataset_base_path}\")\n",
                "print(f\"  Metadata Path: {dataset_metadata_path}\")\n",
                "print(f\"  Output Path: {output_path}\")\n",
                "print(f\"  Model ID: {model_id}\")\n",
                "print(f\"  Epochs: {num_epochs}\")\n",
                "print(f\"  Learning Rate: {learning_rate}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Run Training\n",
                "Execute the training script with the configured parameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!chmod +x train.sh\n",
                "!./train.sh \\\n",
                "  --dataset_base_path \"{dataset_base_path}\" \\\n",
                "  --dataset_metadata_path \"{dataset_metadata_path}\" \\\n",
                "  --output_path \"{output_path}\" \\\n",
                "  --model_id \"{model_id}\" \\\n",
                "  --num_epochs \"{num_epochs}\" \\\n",
                "  --learning_rate \"{learning_rate}\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
